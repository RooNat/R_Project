---
tags: R
Week: 10
type: lecture
lecture: 25
---

## The chapter's focus

The concept of a linear classifier with two examples.
1. **Linear discriminant analysis** (LDA)
	- LDA models the **joint distributions** as **a mixture of Gaussians**.
2. **Logistic regression**
	- Logistic regression models the class conditional distribution directly.


## Linear classification
>Recall the [[24 An introduction to classification#Classification|classification]] and [[24 An introduction to classification#Supervised learning|supervised learning]]
>Linear classification 线性分类

>[!example]- Penguin classification
>![[24 An introduction to classification#Example: Penguin classification|Penguin classification]]

---
**Goal**: to find a function $\phi$ (<font color=#c10b26>classifier</font>, or classification rule) that map a feature vector to a species.
Different types of classifiers are available. One important class of classifiers are <u>linear classifiers</u> 

---
### Linear classification
Let's suppose we want to learn **a binary classifier** $\phi: \mathcal{X}\to\{0,1\}$
Suppose we have $d$ **continuous features** $X=(X^1,\cdots,X^2)\in =\mathbb{R}^d$

>[!important] Linear classifiers
><font color=#c10b26>Linear classifiers</font> are a special type of classifiers that make classification decisions based on **==a linear combination of input==** (features).
>
>More specifically, <font color=#c10b26>a linear classifier</font> $\phi: \mathcal{X}\to\{0,1\}$ cuts the feature space into two with a linear hyperplane. Given a feature vector $X\in \mathcal{X}$, the output $\phi$ completely decided by which side of the <font color=#c10b26>hyperplane</font> that $X$ lies in.
>> - If $d=2$ then the feature is divided by a <font color=#c10b26>line</font>
>> - if $d\geq 2$, the feature space is divided by a $(d-1)$ dimensional hyper-plane (also called the <font color=#c10b26>decision boundary</font>)
>> ![upgit_20221211_1670752931.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221211_1670752931.png)

1. A <font color=#c10b26>linear classifier</font> $\phi: \mathcal{X}\to\{0,1\}$ is of the form
	$$\phi(X)=\begin{cases}
	0 \quad & \text{if} \: w_0+w_1x^1+\cdots+w^dx^d <0 \\
	1 \quad & \text{if} \: w_0+w_1x^1+\cdots+w^dx^d \geq 0
	\end{cases}$$
	1. where $w^0,\cdots, w^d$ are <font color=#c10b26>parameters of classifier</font> $\phi$. 
	2. here $w_0+w_1x^1+\cdots+w^dx^d$ is a <font color=#c10b26>linear combination of the features</font>.
	3. **==weight==**: $\omega:=(\omega^1,\cdots,\omega^d)$.
	4. **==bias==**: $\omega^0$
		![upgit_20221211_1670753402.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221211_1670753402.png)
2. We can rewrite $\phi$ into the following form:
>[!important]
>For vectors $a:=(a^1,\cdots,a^d)$ and $b:=(b^1,\cdots,b^d)$, define $ab^T:=a^1b^1+\cdots+a^d b^d$.
>Then the classifier is rewritten as
>$$\phi(X)=\mathbb{1}\left\{wx^T+w_0\geq 0\right\}$$

3. To learn $\phi$ we need to decide the parameters $\textcolor{blue}{\omega,\omega_0}$.
	What's a good learning algorithm for linear classification?
	- <u>Linear discriminant analysis</u>
	- <u>Logistic regression</u>
	- Others...

## Linear discriminant analysis
>To find a linear classifier, we can use the idea of linear discriminant analysis.

>[!info] The main ideas
>1. Assume some [[15 Foundations of statistical estimation_ Consistency,bias and variance#Probabilistic model(概率模型)|probabilistic model]] (with unknown parameters) for our data
>	- for each of the classes, the feature vectors are generated by a <font color=#c10b26>multivariate Gaussian distribution</font>
>2. Under such an assumption, the <font color=#c10b26>optimal classifier</font>(Bayes classifier) is a <u>linear classifier</u>. We can derive this classifier based on the **probabilistic model**.
>3. Finally, the <font color=#c10b26>parameters</font> of the probabilistic model can be deter mined by the **==maximum likelihood approach==**.

#### 1. Probabilistic model for the data
Suppose we want to learn a classifier to distinguish between <font color=#c10b26>Gentoo</font> and <font color=#c10b26>Adelie penguins</font> (given features like <u>flipper lengths</u>).

| Density of flipper lengths (for all penguins in the sample) | Density of flipper lengths for each of the two classes <br>The data associated with each of the two individual classes looks quite Gaussian. |
| ----------------------------------------------------------- | ------------------------------------------------------ |
| ![[../../../Attachments/Pasted image 20221211115728.png]]   | ![[../../../Attachments/Pasted image 20221211115822.png]]                                                       |

1. **The idea within linear discriminant analysis is:**
	- Let's fit a **Gaussian distribution** to the data from each of the two classes.
	- Then we obtain a probabilistic model for the data.We can then use this **probabilistic model** to generate a rule based on the probability of the <u>two classes</u>.
2. **The linear discriminant  analysis model**
	<u><b>Aim</b></u>: Build a probabilistic model for the data generation process for $(X,Y)\in \mathbb{R} \times \{0,1\}$.
	1. The binary labels $Y\in \{0,1\}$ are modelled as [[12 Random variables#Bernoulli distribution and Bernoulli random variables|Bernoulli random variables]]
		$$Y\sim \mathcal{B}(q) \quad \text{for some fixed} \quad q\in [0,1]$$
		- Recall the **Bernoulli random variables**:
			The **probability mass function** is given by:
			$$p_X(x)=\begin{cases}
			1-q \quad & \text{if} \quad x=0, \\
			q \quad & \text{if} \quad x=1,\\
			0 \quad & \text{otherwise}.
			\end{cases}$$
			![[13 Discrete Random variables#^abe903]]
			![[13 Discrete Random variables#^5431bb]]
		- In the end, the binary labels modelled as Bernoulli random variables:
			$$\mathbb{P}(Y=y)=\begin{cases}
			q \quad & \text{if} \quad y=1,\\
			1-q \quad & \text{if} \quad y=0.
			\end{cases}$$
	2. The feature vectors $X\in \mathbb{R}^d$ are modelled as class <font color=#c10b26>conditional Gaussians</font>.
		$$X\sim \mathcal{N}(\mu_0,
		\Sigma) \quad \text{if} \quad Y=0$$
		$$X\sim \mathcal{N}(\mu_1,
		\Sigma) \quad \text{if} \quad Y=1$$
		where $\mu_0,\mu_1 \in \mathbb{R}^d$ and $\Sigma \in \mathbb{R}^{d\times d}$.
		![upgit_20221211_1670760723.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221211_1670760723.png)
		- Recall **Multivariate Gaussian**:
			![[23 Parameter estimation for multivariate distributions#Multivariate Gaussians]]
		- The feature vectors are modelled as class conditional Gaussians.
			$$f(X=x|Y=y)=\dfrac{1}{\sqrt{(2\pi)^d |\Sigma|}}\mathrm{exp}\left(\dfrac{1}{2}(x-\mu_y)\Sigma^{-1}(x-\mu_y)^T\right) \quad \text{for} \: y\in\{0,1\}$$
			
>[!question] 
>With the probabilistic model, we will discuss in the next steps:
>1. Assuming that the <font color=#c10b26>parameters of the probabilistic model</font> are known, what is the <font color=#c10b26>best classifier</font>? 
>2. How to <font color=#c10b26>estimate the parameters</font> of the probabilistic model from the data?

#### 2. The optimal classifier for the LDA model
>Let's show that the Bayes classifier for LDA model is a <font color=#c10b26>linear classifier</font>.

>[!info]- Recall the Bayes classifier:
>![[24 An introduction to classification#The Bayes classifier]]

$$\begin{align}\phi ^{*}(x) &=1 \\
&\Longleftrightarrow \mathbb{P}(Y=1|X=x)\geq \mathbb{P}(Y=0|X=x) \\
&\Longleftrightarrow \dfrac{\mathbb{P}(X=x|Y=1)\cdot \mathbb{P}(Y=1)}{\mathbb{P}(X=x)} \geq \dfrac{\mathbb{P}(X=x|Y=0)\cdot \mathbb{P}(Y=0)}{\mathbb{P}(X=x)} \\
\end{align}$$
**This is a consequence of the Bayes theorem:**
$$\mathbb{P}(Y=y|X=x)=\dfrac{\mathbb{P}(X=x|Y=y)\cdot \mathbb{P}(Y=y)}{\mathbb{P}(X=x)}$$
$$\begin{align}\phi ^{*}(x) &=1 \\
&\Longleftrightarrow \mathbb{P}(X=x|Y=1)\cdot \mathbb{P}(Y=1) \geq \mathbb{P}(X=x|Y=0) \cdot \mathbb{P}(Y=0) \\
&\Longleftrightarrow \text{exp}\left(-\dfrac{1}{2}(x-\mu_1)\Sigma^{-1} (x-\mu_1)^T\right) \cdot q \geq \text{exp}\left(-\dfrac{1}{2}(x-\mu_0) \Sigma^{-1} (x-\mu_0)^T\right) \cdot (1-q) \\
&\Longleftrightarrow -\dfrac{1}{2}(x-\mu_1)\Sigma^{-1}(x-\mu_1)^T+\text{log}q \geq -\dfrac{1}{2}(x-\mu_0)\Sigma^{-1}(x-\mu_0)^T +\text{log}(1-q) \\
&\Longleftrightarrow x\Sigma^{-1}(\mu_1-\mu_0)^T+\left\{\text{log}\left(\dfrac{q}{1-q}\right)-\dfrac{1}{2}\left(\mu_1\Sigma^{-1} \mu_1^T-\mu_0\Sigma^{-1}\mu_0^T\right)\right\} \geq 0 \\
&\Longleftrightarrow x\omega^T+\omega_0 \geq 0 \\
& \text{where} \quad \omega:=\Sigma^{-1}(\mu_1-\mu_0)^T, \omega_0:=\text{log}\left(\dfrac{q}{1-q}\right)-\dfrac{1}{2}\left(\mu_1\Sigma^{-1} \mu_1^T-\mu_0\Sigma^{-1}\mu_0^T\right)
\end{align}$$
>[!important]
>Therefore, the Bayes classifier $\phi^{*}(x)=\mathbb{1}\left\{\omega x^T+\omega_0 \geq 0\right\}$ with 
>$$\omega:=(\mu_1-\mu_0)\Sigma^{-1},$$
>$$\omega_0:=\text{log}\left(\dfrac{q}{1-q}\right)-\dfrac{1}{2}\left(\mu_1\Sigma^{-1} \mu_1^T-\mu_0\Sigma^{-1}\mu_0^T\right)$$

**Note**: 
1. The Bayes classifier in the linear discriminant model is <font color=#c10b26>linear</font>!
2. However, we don't know the parameters $q,\mu_0,\mu_1,\Sigma$. They need to be computed from the data.

#### 3. Parameter estimation for LDA
>How can we learn the parameters $q,\mu_0,\mu_1,\Sigma$ for the <font color=#c10b26>linear discriminant analysis</font> model?
>We can fit **the probabilistic model** to the data, by using [[16 An introduction to maximum likelihood estimation#Maximum likelihood estimation|the maximum likelihood principle]]!

1. For simplicity let's look at the <font color=#c10b26>one-dimensional case</font> $d=1$. #fleeting 
	$$\mathbb{P}(Y=y)=\begin{cases}
	q &\quad \text{if} \quad y=1,\\
	1-q & \quad \text{if} \quad y=0.
	\end{cases} \quad =q^y(1-q)^{1-y}$$
	$$\begin{align}
	\mathbb{P}(X=x|Y=y) &=\dfrac{1}{\sqrt{(2\pi)^d |\Sigma|}}\mathrm{exp}\left(\dfrac{1}{2}(x-\mu_y)\Sigma^{-1}(x-\mu_y)^T\right) \\
	&= \dfrac{1}{\sqrt{(2 \pi) \sigma^2}} \mathrm{exp}\left(\dfrac{1}{2}(x-\mu_y)^2/(2\sigma^2)\right)
	\end{align}$$
	
2. Compute the likelihood of the data $\mathcal{D}=((X_1,Y_1),\cdots,(X_n,Y_n))$
	$$\begin{align}
	\mathbb{P}(X=x,Y=y) &=\mathbb{P}(Y=y)\cdot\mathbb{P}(X=x|Y=y) \\
	&=q^y(1-q)^{1-y} \cdot \left\{\dfrac{1}{\sqrt{2\pi \sigma^2}}\text{exp}\left(-\dfrac{1}{2\sigma^2}\left(x-\mu_y\right)^2\right)\right\}
	\end{align}$$
	- Assuming that $(X_i,Y_i)$ are independent, **the likelihood function** is given by:
	$$\begin{align}
	l(q,\mu_0,\mu_1,\sigma) &=\prod_{i=1}^n\mathbb{P}(X=X_i,Y=Y_i) \\
	&=\prod_{i=0}^{n}\left\{q^{Y_i}(1-q)^{1-Y_i} \cdot \left\{\dfrac{1}{\sqrt{2\pi \sigma^2}}\text{exp}\left(-\dfrac{1}{2\sigma^2}\left(X_i-\mu_{Y_i} \right)^2\right)\right\}\right\}
	\end{align}$$
	- To maximise the likelihood we will maximise the log-likelihood.
		![upgit_20221211_1670781140.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221211_1670781140.png)

	- To maximise the **log-likelihood** we find the point where the **derivatives** are equal to zero.
		1. derivative with respect to $q$.
			$$\dfrac{\partial}{\partial q}\text{log}l(q,\mu_0,\mu_1,\sigma)=(\sum_{i=1}^{n}Y_i)\cdot \left\{\dfrac{1}{q}+\dfrac{1}{1-q}\right\}-\dfrac{n}{1-q}$$
			Taking $\dfrac{\partial}{\partial q}\text{log}l(q,\mu_0,\mu_1,\sigma)=0$ , give an MLE of $\hat q=\dfrac{1}{n}\sum Y_i$. 
		2. The derivative with respect to $\mu_0$ is:
			$$\dfrac{\partial}{\partial \mu_0}\text{log}l(q,\mu_0,\mu_1,\sigma)=\dfrac{1}{\sigma^2}\sum_{\{i:Y_i=0\}}(X_i-\mu_0)$$
			1. Taking $\dfrac{\partial}{\partial \mu_0}\text{log}l=0$ gives an MLE of $\hat \mu_0=\dfrac{1}{n_0}\sum_{\{i:Y_i=0\}}X_i$ where $n_0:=|\{i:Y_i=0\}|$.
			2. The MLE for $\mu_1$ is $\hat \mu_1=\dfrac{1}{n_1}\sum_{\{i:Y_i=1\}}X_i$ where $n_1:=|\{i:Y_i=0\}|$.
		3. The derivative with respect to $\sigma$ is:
			$$\dfrac{\partial}{\partial \sigma}\text{log}l=-\dfrac{n}{\sigma}+\dfrac{1}{\sigma^3}\left\{\sum_{\{i:Y_i=0\}}\{X_i-\mu_0\}^2+\sum_{\{i:Y_i=1\}}\{X_i-\mu_1\}^2\right\}$$
			So the MLE is $\hat \sigma^2=\dfrac{1}{n}\left\{\sum_{\{i:Y_i=0\}}\{X_i-\hat \mu_0\}^2+\sum_{\{i:Y_i=1\}}\{X_i-\hat \mu_1\}^2\right\}$
		4. The **maximum likelihood estimates**:
			Given data $\mathcal{D}=((X_1,Y_1),\cdots,(X_n,Y_n))$, the MLE of the parameters is:
			$$\hat q=\dfrac{1}{n}\sum Y_i$$
			$$\hat \mu_0=\dfrac{1}{n_0}\sum_{\{i:Y_i=0\}}X_i \quad \text{where} \quad n_0:=|\{i:Y_i=0\}|$$
			$$\hat \mu_1=\dfrac{1}{n_1}\sum_{\{i:Y_i=1\}}X_i \quad \text{where} \quad n_1:=|\{i:Y_i=1\}|$$
			$$\hat \Sigma_{\text{MLE}}=\dfrac{1}{n}\left\{\sum_{\{i:Y_i=0\}}\{X_i-\hat \mu_0\}\{X_i-\hat \mu_0\}^T+\sum_{\{i:Y_i=1\}}\{X_i-\hat \mu_1\}\{X_i-\hat \mu_1\}^T \right\}$$
			**Note**: The $\hat \Sigma_{MLE}$ is <font color=#c10b26>biased</font>.
3. **==The linear discriminant analysis classifier==**. 
	The linear discriminant analysis classifier is given by $\hat \phi(x)=\mathbb{1}\left\{\omega x^T+\hat \omega_0 \geq 0\right\}$
	with $\hat \omega:=(\hat \mu_1-\hat \mu_0)\Sigma_{\text{MLE}}^{-1}$ , $\hat \omega_0:=\text{log}\left(\dfrac{\hat q}{1-\hat q}\right)-\dfrac{1}{2}\left(\hat \mu_1 \hat \Sigma_{U}^{-1} \hat \mu_1^T-\hat \mu_0\Sigma_{U}^{-1}\hat \mu_0^T\right)$ 
	where
	$$\hat q=\dfrac{1}{n}\sum Y_i$$
	$$\hat \mu_0=\dfrac{1}{n_0}\sum_{\{i:Y_i=0\}}X_i \quad \text{where} \quad n_0:=|\{i:Y_i=0\}|$$
	$$\hat \mu_1=\dfrac{1}{n_1}\sum_{\{i:Y_i=1\}}X_i \quad \text{where} \quad n_1:=|\{i:Y_i=1\}|$$
	$$\hat \Sigma_{\text{U}}=\dfrac{1}{n-2}\left\{\sum_{\{i:Y_i=0\}}\{X_i-\hat \mu_0\}\{X_i-\hat \mu_0\}^T+\sum_{\{i:Y_i=1\}}\{X_i-\hat \mu_1\}\{X_i-\hat \mu_1\}^T \right\}$$
	**Note**: here we considered an alternative of the MLE: <font color=#c10b26>the unbiased estimate</font> $\hat \Sigma_U$.

#### 4. The linear discriminant analysis classifier in R

>[!example]
><u>Example</u>: Suppose we want to learn a classifier $\phi: \mathcal{X}\to \mathcal{Y}$ which takes a feature vector of morphological features and predicts whether a penguin belongs to either the <font color=#c10b26>Adelie species</font> or the <font color=#c10b26>Gentoo species</font>.

1. **prepare the data**
	```r
	library(tidyverse)
	library(palmerpenguins)
	peng_total<-penguins%>%
	  filter(species!="Chinstrap")%>%
	  select(species,body_mass_g,flipper_length_mm)%>%
	  drop_na()%>%
	  mutate(species=as.numeric(species=="Adelie"))
	peng_total
	```
	![upgit_20221211_1670793240.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221211_1670793240.png) ^2b5da6
	- **Features**: $X=(X^1,X^2) \in \mathcal{X} :=\mathbb{R}^2$
		- $X^1= \text{the weight of the penguin (grams)}$
		- $X^2=\text{the flipper length of the penguins (mm)}$
	- **Labels**: $Y\in \mathcal{Y} :=\{0,1\}$
		$$Y=\begin{cases}
	1 \quad & \text{if the penguin is an Adelie} \\
	0 \quad & \text{if the penguin is a Gentoo}
	\end{cases}$$

2. **Carry out a train test split**:
	![[24 An introduction to classification#Carry out a train test spilt:]]
 ^fd7c91
3. Build <font color=#c10b26>a linear discriminant analysis</font> model with R as follows:
	```r
	lda_model<-MASS::lda(species ~.,data=peng_train) # fit LDA model
	lda_model
	```
	**Results**:
	```
	Call:
	lda(species ~ ., data = peng_train)
	Prior probabilities of groups:
        0         1 
    0.4634146 0.5365854 
    
    Group means:
	    body_mass_g flipper_length_mm
	0    5035.789          217.1474
	1    3696.136          190.5636
	
	Coefficients of linear discriminants:
                            LD1
	body_mass_g       -9.446313e-05
	flipper_length_mm -1.508201e-01
	```
	
4. Make **predictions** and check the <font color=#c10b26>training error</font> as follows:
	```r
	lda_train_predicted_y<-predict(lda_model,peng_train_x)$class%>%
		as.character()%>%as.numeric() #get vector of predicted ys
	lda_train_error<-mean(abs(lda_train_predicted_y-peng_train_y)) #compute train error
	lda_train_error
	```
	**Results**:
	```
	[1] 0.01463415
	```
5. We can make predictions and check the <font color=#c10b26>test error</font> as follows:
	```r
	lda_test_predicted_y<-predict(lda_model,peng_test_x)$class%>%
		as.character()%>%as.numeric() #get vector of predicted ys
		lda_test_error<-mean(abs(lda_test_predicted_y-peng_test_y)) #compute train error
		lda_test_error
	```
	**Results**:
	```
	[1] 0.01449275
	```
	
## Logistic regression
>逻辑回归

>[!note]
>To find a linear classifier ——use the idea of **logistic regression**
>1. logistic regression is that its not <font color=#c10b26>a regression algorithm</font>!
>2. The <font color=#c10b26>logistic regression approach</font> assumes a different probabilistic model (from the LDA model).
>3. Then, under such an assumption, the <font color=#c10b26>optimal classifier</font> (Bayes classifier) is <font color=#c10b26>a linear classifier</font>. We can derive this classifier based on the probabilistic model.
>4. Finally, the <font color=#c10b26>parameters</font> of the probabilistic model can be determined by the <font color=#c10b26>maximum likelihood approach</font>. 

#### 1. Logistic regression probabilistic model
>Logistic regression is a method for learning a linear classifier $\phi(x)=\mathbb{1}(wx^T+\omega _0 \geq 0)$
>We can consider construction the conditional probability by using $\omega x^T+\omega_0$

>[!important]
>Use the sigmoid function $S:\mathbb{R}\to (0,1)$ to map real numbers to probabilities: $S(z):=\dfrac{1}{1+e^z}$.
>**Facts**:
>1. $1-S(z)=S(-z)$, i.e., symmetric about $(0,\dfrac{1}{2})$.
>2. $\dfrac{\partial \text{log}S(z)}{\partial z}=S(-z)$
>3. $S(z)\geq \dfrac{1}{2}$ if and only if $z\geq 0$.
>
>We use the logistic sigmoid model:
>$$\mathbb{P}(Y=1|X=x)=S(\omega x^T+\omega ^0)=\dfrac{1}{1+e^{- \omega x^T-\omega^0}}$$


![[../../../Attachments/Pasted image 20221212115707.png]]
#### 2. The optimal classifier for the logistic model
>**Note**: with the probabilistic model, we can model the **optimal classifier**
>Recall the Bayes classifier
>Next, justify that under the **logistic sigmoid model**, the **Bayes classifier** is **linear classifier**.

$$\begin{align}\phi^{*}(x) &=1 \\
&\Longleftrightarrow \mathbb{P}(Y=1|X=x) \geq \mathbb{P}(Y=0|X=x) \\
&\Longleftrightarrow \mathbb{P}(Y=1|X=x) \geq \dfrac{1}{2} \\
&\Longleftrightarrow S(\omega x^T+\omega ^0) \geq \dfrac{1}{2} \\
&\Longleftrightarrow \omega x^T+ \omega ^0 \geq 0
\end{align}$$
Note that $\mathbb{P}(Y=1|X=x)+\mathbb{P}(Y=0|X=x)=1$
**Conclusion**: $\phi ^{*}(x)=\mathbb{1}(\omega x^T+ \omega ^0 \geq 0)$

>So the Bayes classifier for the logistic model is a linear classifier.
>The parameters $\omega$, $\omega_0$ are unknown.
>We want to learn the parameters from the data. Then based On the learned parameters, we can construct the Bayes classifier.

#### 3. Learning parameters for a logistic model
>We can learn the parameters $\omega,\omega_0$ of the model with the maximum likelihood principle.

1. First, we have: #fleeting 
	$$\begin{align}\mathbb{P}(Y=y|X=x)
	&=\begin{cases}
	S(\omega x^T+\omega ^0) \quad & \text{if} \quad y=1 \\
	1-S(\omega x^T+\omega^0)=S(-\omega x^T - \omega ^0) \quad & \text{if}  \quad y=0
	\end{cases} \\ \\
	&=S\left\{(2y-1)\cdot (\omega x^T +\omega ^0)\right\}
	\quad \quad 2y-1=1 \: \text{if} \: y=1 \: \text{and} \: 2y-1=-1 \:\text{if} \: y=0
	\end{align}$$
2. Second, suppose we have training data $\mathcal{D}=((X_1,Y_1),\cdots,(X_n,Y_n))$, the likelihood function is given by
	$$l(\omega,\omega^0)=\prod_{i=1}^nS\left\{(2Y_i -1)\cdot \left(\omega X_i ^T+\omega^0\right)\right\}$$
	The log-likelihood function is :
	$$\text{log}l(\omega,\omega^0)=\sum_{i=1}^n \text{log} S\left\{(2Y_i -1)\cdot \left(\omega X_i ^T+\omega^0\right)\right\}$$
3. To maximise the log-likelihood we compute the gradients.
	$$\dfrac{\partial}{\partial \omega}\text{log}l(\omega,\omega^0)=\sum_{i=1}^n 2(Y_i-1)S\left\{(1-2Y_i)\cdot \left(\omega X_i^T +\omega^0\right)\right\} \cdot X_i$$
	$$\dfrac{\partial}{\partial \omega^0}\text{log}l(\omega,\omega^0)=\sum_{i=1}^n 2(Y_i-1)S\left\{(1-2Y_i)\cdot \left(\omega X_i^T+\omega^0\right)\right\}$$
	Unfortunately for logistic regression, there is no analytic solution for the MLE.
	We can <font color=#c10b26>maximise the function numerically</font>.
4. We maximise the log-likelihood $\text{log}l(\omega,\omega_0)$ iteratively.
	We can use **a process of gradient ascent**:
	$$\omega \gets \omega+ \alpha \cdot \dfrac{\partial}{\partial \omega}\text{log}l(\omega,\omega^0)$$
	$$\omega^0 \gets \omega^0 +\alpha \cdot \dfrac{\partial}{\partial \omega^0}\text{log}l(\omega,\omega^0)$$
	![upgit_20221212_1670846130.png](https://raw.githubusercontent.com/RooNat/Myimages/main/2022/12/upgit_20221212_1670846130.png)

#### 4. Logistic regression with R

>[!example]
><u>Example</u>: Suppose we want to learn a classifier $\phi: \mathcal{X}\to \mathcal{Y}$ which takes a feature vector of morphological features and predicts whether a penguin belongs to either the <font color=#c10b26>Adelie species</font> or the <font color=#c10b26>Gentoo species</font>.

![[#^2b5da6]]

![[#^fd7c91]]

- We can train a logistic model with the `glmnet` library.
	```r
	library(glmnet) 
	logistic_model<-glmnet(x=peng_train_x%>%as.matrix(),
                       y=peng_train_y,family="binomial",
                       alpha=0,lambda=0) #train a logistic model
	```
- We compute the training error as follows.
	```r
	logistic_train_predicted_y<-predict(logistic_model,peng_train_x%>%
                                      as.matrix(),type="class")%>%as.integer()
    logistic_train_error<-mean(abs(logistic_train_predicted_y-peng_train_y)) #train error
    logistic_train_error
	```
	
	**Results**:
	```
	[1] 0.009756098
	```

- **Test error**:
	```r
	logistic_test_predicted_y<-predict(logistic_model,peng_test_x%>%
                                     as.matrix(),type="class")%>%as.integer()
    logistic_test_error<-mean(abs(logistic_test_predicted_y-peng_test_y)) #test error
    logistic_test_error
	```
	**Results**:
	```
	[1] 0.01449275
	```